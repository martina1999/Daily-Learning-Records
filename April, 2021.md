# Python语法

### json格式转换（4/15）

- loads：string转换为dict
- dumps：dict转换为string
- load：json格式字符串转换为dict（读取文件）
- dump：dict转换为json格式字符串（存入文件）



遍历每个case

​	遍历case中context的每个段落（只有一段）

​		遍历段落中的每个句子

​			遍历句子中的每个字符——将每个单词存入doc_tokens，将

​			

### argparse模块（4/15）

> Python 内置的一个用于命令项选项与参数解析的模块，通过在程序中定义好我们需要的参数，argparse 将会从 sys.argv 中解析出这些参数，并自动生成帮助和使用信息。当然，Python 也有第三方的库可用于命令行解析，而且功能也更加强大，比如 `docopt`，`Click`。

- python命令行参数（sys.argv）

  - 写一些脚本处理任务时候往往需要提供一些命令行参数，在Python里，命令行的参数和C语言很类似（因为标准Python是用C语言实现的）。在C语言里，main函数的原型为int main(int argc, char ** argv)，这里主要指linux平台， argc指的是命令行传入的参数个数（程序的name为第一个参数），而argv则是一个指针数组，每一个元素为指向一个命令行参数的指针。**在Python里的命令行参数是存储在sys.argv里，argv是一个列表，第一个元素也为程序名称**。
- 使用步骤：
  - import argparse 首先导入模块
  - parser = argparse.ArgumentParser（） 创建一个解析对象
  - parser.add_argument() 向该对象中添加你要关注的命令行参数和选项
  - args = parser.parse_args() 进行解析
  - 在需要时使用args.XXX



### 关于str的一些函数用法（4/15）

1. `str.find(str, beg=0, end=len(string))`

   Python find() 方法检测字符串中是否包含子字符串 str ，如果指定 beg（开始） 和 end（结束） 范围，则检查是否包含在指定范围内，如果包含子字符串返回开始的索引值，否则返回-1。

2. `str.join(sequence)`

   用于将序列中的元素以指定的字符连接生成一个新的字符串



### 关于list的一些函数用法（4/15）

1. `list.append(obj)`向列表中添加一个**对象**obj
2. `list.extend(sequence)`把一个**序列**seq的内容添加到列表中



# NLP模型实现

### transfromers包

> 官方文档<https://pypi.org/project/transformers/>

1. 特点（4/15）

   提供了对成千上万的预训练模型可用于多种NLP任务，提供了APIs来快速下载并使用这些在给定文本上的预训练模型，可以将它们在自己的数据集上进行微调；

   每个python module可以被独立使用和调整；

   支持两种最流行的深度学习框架：PyTorch和TensorFlow，并且可以无缝集成

2. 1

# 深度学习知识整理

### 机器学习基础

> 花书C5

#### 容量、过拟合和欠拟合（4/16）

#####   1 泛化与误差

- **泛化**：在先前未观测到的输入上表现良好的能力
- 2种误差：
  - 训练误差
  - 泛化误差（测试误差）
- 机器学习和优化不同的地方在于也希望泛化误差很低

##### 2 数据生成过程、独立同分布假设

- 随机模型训练误差的期望和该模型测试误差的期望是一样的
- 实际使用ML算法时不会提前固定参数，然后采样得到两个数据集
  1. 采样得到训练集
  2. 挑选参数去降低训练集误差
  3. 采样得到测试集

##### 3 欠拟合和过拟合

- 决定ML算法效果的两个因素

  - 降低训练误差
  - 缩小训练误差和测试误差之间的差距

- 如何解决？——调整模型的**容量**（两种方法）

  - 改变输入特征的数目
  - 改变加入这些特征对应的参数

- 表示容量、有效容量、奥卡姆剃刀

- 量化模型的容量——VC维

- **通常泛化误差是一个关于模型容量的U型曲线函数**

- 非参数模型（eg最近邻回归）

- **<u>没有免费午餐定理</u>**

  > 没有最优的学习算法
  >
  > ML的目标不是找一个通用学习算法，而是什么样的学习算法在关注的数据生成分布上效果最好

- **<u>正则化</u>**
  - 表明偏好
  - eg（偏好）权重衰减：最小化损失函数可看做拟合训练数据和偏好小权重范数之间的权衡
  - 修该学习算法，使其降低泛化误差而非训练误差



### 卷积网络

> 花书C9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     

#### 卷积运算（4/16）

> 原输出x(t)，受噪声干扰，为了低噪声估计，对结果加权平均，对最近的测量结果赋予更高权重
>
> $s(t) = \int x(a)w(t-a)da$

$s(t) = (x * w)(t)$

> 两个参数：x——输入；w——核函数
>
> 输出——特征映射

$s(t) = (x*w)(t) = \displaystyle\sum^{\infty}_{a=-\infty}{x(a)w(t-a)}$

> 离散形式的卷积

核的大小通常远小于输入图像的大小

#### 在神经网络中使用卷积运算的动机（4/16）

- 三个重要思想：

  - 稀疏交互（sparse interactions）

    > 也叫稀疏连接或稀疏权重，不需要每一个输出单元与每一个输入单元都产生交互（vs矩阵乘法）

  - 参数共享（parameter sharing）

    > 绑定的权重

  - 等变表示（equivariant representations）
    - 卷积对平移等变（由于参数共享）
    - 卷积对其他的一些变换并不是天然等变的（eg图像的放缩、旋转变换），需要其他的一些机制来处理这些变换

#### 池化（4/16）

- 卷积网络中一个典型层包含三级：
  - 卷积级：并行地计算多个卷积产生一组线性激活响应
  - 探测级：每个线性激活响应将会通过一个非线性的激活函数（eg整流线性激活函数）
  - 池化级：用池化函数（pooling function）来进一步调整这一层的输出
    - 池化函数：使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出
      - 最大池化函数：给出相邻矩形区域内的最大值
      - 相邻矩形区域内的平均值
      - $L^2$范数
      - 基于距中心像素距离的加权平均函数
    - “池的宽度”，“池的步幅”
- 输入做出少量平移时，池化能够帮助输入的表示近似不变——可将使用池化看作增加了一个无限强的先验：这一层学得的函数必须具有**对少量平移的不变性**
- 当我们对分离参数的卷积的输出进行池化时，特征能够学得应该对于哪种变换具有不变性
- 池化单元可以少于探测单元（**带有降采样的池化**）——能提高统计效率、减少对参数的存储需求，还可以处理不同大小的输入（eg分类，可以使分类层接收到相同数量的统计特征而不管最初的输入大小）

  

# NLP理论学习

### 机器翻译（4/15）

统计机器翻译需要学习两个模型：

1）翻译模型——从平行语料中学习如何翻译单词/短语

2）语言模型——从单语语料中学习如何生成流畅语言

神经机器翻译则没有以上两个模型，而是使用神经网络进行**端到端**的文本翻译

#### Seq2Seq模型



### 预训练语言模型（4/15）

两种：

1. 基于特征的方法

> eg. word2vec，预训练模型的输出作为下游任务模型的输入

2. 基于微调的方法

> eg. BERT，语言模型本身也是下游任务模型的一部分，参数随着训练一起更新



#### 基于特征的方法（4/15）

1. word2vec——缺：每个词只对应一个固定的、上下文无关的词向量

2. context2vec——词向量只和当前的上下文有关，但是忽略了词本身

   使用单层的Bi-LSTM对上下文信息进行编码，而不是简单的使用上下文词向量的平均

3. ELMo——上下文相关，网络深，字符级别输入（解决OOV）

   将每一层的隐状态结合在一起作为结合上下文的词向量



#### 基于微调的方法

> RNN存在梯度消失和梯度爆炸问题使其难以训练——两种无监督训练方法来改进：
>
> 1. 传统的语言模型
> 2. **序列自编码器（sequence autoencoder）**



ans_start_position

ans_end_position

para_start_end_position



